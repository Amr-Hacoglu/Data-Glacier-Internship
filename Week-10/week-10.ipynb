{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9136481,"sourceType":"datasetVersion","datasetId":5517570}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 10 Deliverables","metadata":{}},{"cell_type":"markdown","source":"**Team Member's Details**\n\nGroup Name: Data Science Bank Marketers\nMembers:\n\nAmr Hacoglu – amr.hacoglu@gmail.com - Turkey - University of Karabuk - Data Science\nHa My Pham – mpham25@wooster.edu - US – College of Wooster – Data Science","metadata":{}},{"cell_type":"markdown","source":"**Problem Description**\n\nABC Bank aims to develop a machine learning model to predict whether a customer will subscribe to a term deposit product. This model will help the bank focus its marketing efforts on customers with a higher likelihood of purchasing the product, thereby optimizing resource allocation and reducing marketing costs.","metadata":{}},{"cell_type":"markdown","source":"**Exploratory Data Analysis (EDA)**\n\n* Analyzed the distribution of features and their relationships with the target variable.\n* Handled missing values using mean/median/mode imputation and model-based approaches.\n* Identified and addressed outliers using the IQR method and capping.\n* Performed feature engineering to create new features and transform existing ones.\n* Addressed the class imbalance issue using SMOTE, class weighting, and undersampling techniques.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Final Recommendation**\n\nBased on the EDA and feature engineering, we recommend using a combination of Logistic Regression, ensemble methods (e.g., Random Forest), and boosting algorithms (e.g., XGBoost, LightGBM) to build the predictive model. To evaluate the model's performance, we suggest using appropriate metrics such as AUC-ROC, precision, recall, and F1-score. Additionally, we can translate the model's performance into business metrics like potential cost savings and increased conversion rates.","metadata":{}},{"cell_type":"markdown","source":"**GitHub Repository**\n\nhttps://github.com/Amr-Hacoglu/Data-Glacier-Internship","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom statistics import mean\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and inspect the dataset\ndf = pd.read_csv('/kaggle/input/bankdataset/bank-additional-full.csv', sep=';')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle missing values\nnumeric_columns = df.select_dtypes(include=[np.number]).columns\ndf[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n\ncategorical_columns = df.select_dtypes(include=['object']).columns\ndf[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify and handle outliers\ndef handle_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    df[column] = df[column].clip(lower_bound, upper_bound)\n\nfor column in numeric_columns:\n    handle_outliers(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\nle = LabelEncoder()\ndf['month'] = le.fit_transform(df['month'])\ndf['day_of_week'] = le.fit_transform(df['day_of_week'])\ndf['poutcome'] = le.fit_transform(df['poutcome'])\ndf['job'] = le.fit_transform(df['job'])\ndf['marital'] = le.fit_transform(df['marital'])\ndf['education'] = le.fit_transform(df['education'])\ndf['default'] = le.fit_transform(df['default'])\ndf['housing'] = le.fit_transform(df['housing'])\ndf['loan'] = le.fit_transform(df['loan'])\ndf['contact'] = le.fit_transform(df['contact'])\ndf['y'] = le.fit_transform(df['y'])\n\nX = df.iloc[:, :20]\ny = df['y']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle class imbalance using SMOTE\noversample = SMOTE()\nover_X, over_y = oversample.fit_resample(X, y)\nover_X_train, over_X_test, over_y_train, over_y_test = train_test_split(over_X, over_y, test_size=0.1, stratify=over_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build SMOTE SRF model\nSMOTE_SRF = RandomForestClassifier(n_estimators=100, random_state=0)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscoring = ('f1', 'recall', 'precision')\nscores = cross_validate(SMOTE_SRF, over_X, over_y, scoring=scoring, cv=cv)\n\nprint('Mean f1: %.3f' % mean(scores['test_f1']))\nprint('Mean recall: %.3f' % mean(scores['test_recall']))\nprint('Mean precision: %.3f' % mean(scores['test_precision']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\nSMOTE_SRF.fit(over_X_train, over_y_train)\ny_pred = SMOTE_SRF.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Will Not Buy', 'Will Buy'])\ndisp.plot(cmap='Greens')\nplt.title('SMOTE + Standard Random Forest Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract feature importances\nfeature_importances = SMOTE_SRF.feature_importances_\nfeatures = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nfeatures = features.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(12, 8))\nplt.barh(features['Feature'], features['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances from RandomForestClassifier')\nplt.gca().invert_yaxis()\nplt.show()\n\nfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=df[['duration','euribor3m','nr.employed','cons.conf.idx','cons.price.idx']]\nprint(features)\nModel = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=3)\nModel.fit(features,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import plot_tree\n# Extract a single tree from the forest (e.g., the first tree)\ntree = Model.estimators_[0]\n\n# Plot the tree\nplt.figure(figsize=(20,10))\nplot_tree(tree, filled=True, feature_names=features.columns, class_names=['Will Not Buy', 'Will Buy'], rounded=True)\nplt.title('Decision Tree from RandomForestClassifier')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}